{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### `Imports`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import glob\n",
    "import json\n",
    "from functools import reduce, partial\n",
    "from sql_metadata import Parser\n",
    "import sqlparse\n",
    "import re\n",
    "import logging\n",
    "\n",
    "logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### `MAIN extract_queries_and_meta_data`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### `Methods`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-04-07 11:14:30,599 - INFO - Started extract_queries_and_meta_data.\n",
      "2025-04-07 11:14:30,601 - INFO - Started read_in_excel_files from path ./1_migrated_excel_queries/test.xlsx.\n",
      "2025-04-07 11:14:30,616 - INFO - Successfully read 1 excel file(s) from ./1_migrated_excel_queries/test.xlsx.\n",
      "2025-04-07 11:14:30,617 - INFO - Started process_query_df.\n",
      "2025-04-07 11:14:30,618 - INFO - Started preprocess_query_df for migrated queries.\n",
      "2025-04-07 11:14:32,561 - INFO - Preprocessing of migrated queries successful.\n",
      "2025-04-07 11:14:32,563 - INFO - Started write_queries_to_json for migrated queries.\n",
      "2025-04-07 11:14:32,592 - INFO - Queries successfully written to JSON-file.\n",
      "2025-04-07 11:14:32,592 - INFO - Started extract_query_meta_data from migrated queries.\n",
      "2025-04-07 11:14:32,594 - INFO - Meta data from migrated queries successfully extracted.\n",
      "2025-04-07 11:14:32,594 - INFO - Started extract_query_tables from migrated queries.\n",
      "2025-04-07 11:14:33,119 - INFO - Tables from migrated queries successfully extracted.\n",
      "2025-04-07 11:14:33,120 - INFO - Started extract_query_columns from migrated queries.\n",
      "2025-04-07 11:14:33,626 - INFO - Columns from migrated queries successfully extracted.\n",
      "2025-04-07 11:14:33,627 - INFO - Started write_query_data_to_json for migrated queries.\n",
      "2025-04-07 11:14:33,630 - INFO - 13 queries successfully written to JSON-file. Path: ./4_json_results/migrated_query_data.json\n",
      "2025-04-07 11:14:33,632 - INFO - Completed process_query_df for migrated queries.\n"
     ]
    }
   ],
   "source": [
    "extract_queries_and_meta_data()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### `extract_queries_and_meta_data`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_queries_and_meta_data():\n",
    "    logging.info(f\"Started {extract_queries_and_meta_data.__name__}.\")\n",
    "\n",
    "    # Migrated Queries\n",
    "    MIGRATED_QUERY_SOURCE_PATH = \"./1_migrated_excel_queries/test.xlsx\"\n",
    "    MIGRATED_QUERY_TARGET_PATH = \"./3_extracted_queries/migrated_queries\"\n",
    "    MIGRATED_QUERY_IDENTIFIER = \"migrated\"\n",
    "\n",
    "    migrated_query_df = read_in_excel_files(MIGRATED_QUERY_SOURCE_PATH)\n",
    "    return process_query_df(migrated_query_df, MIGRATED_QUERY_TARGET_PATH, MIGRATED_QUERY_IDENTIFIER)\n",
    "\n",
    "    # New Queries\n",
    "    # NEW_QUERY_SOURCE_PATH = \"./2_new_excel_queries/*.xlsx\"\n",
    "    # NEW_QUERY_TARGET_PATH = \"./3_extracted_queries/new_queries\"\n",
    "    # NEW_QUERY_IDENTIFIER = \"new\"\n",
    "\n",
    "    # new_query_df = read_in_excel_files(NEW_QUERY_SOURCE_PATH)\n",
    "    # process_query_df(new_query_df, NEW_QUERY_TARGET_PATH, NEW_QUERY_IDENTIFIER)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### `read_in_excel_files`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_in_excel_files(PATH):\n",
    "    \"\"\"\n",
    "    Reads all Excel files from the specified directory path and concatenates them into a single DataFrame.\n",
    "\n",
    "    Parameters:\n",
    "    PATH (str): The directory path where the Excel files are located. Supports glob patterns.\n",
    "\n",
    "    Returns:\n",
    "    pd.DataFrame: A concatenated DataFrame containing the data from all the Excel files, with missing values filled with \"NA\".\n",
    "    \"\"\"\n",
    "    logging.info(f\"Started {read_in_excel_files.__name__} from path {PATH}.\")\n",
    "    try:\n",
    "        excel_data = glob.glob(PATH)\n",
    "\n",
    "        dataframes = [pd.read_excel(data, engine=\"openpyxl\") for data in excel_data]\n",
    "        query_df = pd.concat(dataframes, ignore_index=True)\n",
    "\n",
    "        logging.info(f\"Successfully read {len(excel_data)} excel file(s) from {PATH}.\")\n",
    "\n",
    "    except Exception as e:\n",
    "        logging.error(f\"Error in reading excel files: {e}\")\n",
    "\n",
    "    return query_df.fillna(\"NA\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### `process_query_df`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_query_df(query_df, QUERY_TARGET_PATH, QUERY_IDENTIFIER):\n",
    "    \"\"\"\n",
    "    Processes the query DataFrame by applying a series of transformations and extracting metadata.\n",
    "\n",
    "    Parameters:\n",
    "    query_df (pd.DataFrame): The DataFrame containing query data to be processed.\n",
    "    QUERY_TARGET_PATH (str): The target path where the processed query data will be written as JSON.\n",
    "    QUERY_IDENTIFIER (str): A unique identifier for the queries being processed.\n",
    "\n",
    "    Returns:\n",
    "    None\n",
    "\n",
    "    Steps:\n",
    "    1. Apply preprocessing and write queries to JSON using partial functions.\n",
    "    2. Extract query metadata, tables, and columns from the processed DataFrame.\n",
    "    3. Write the extracted metadata, tables, and columns to JSON files.\n",
    "\n",
    "    Example:\n",
    "    >>> process_query_df(query_df, \"/path/to/target\", \"example_query\")\n",
    "    \"\"\"\n",
    "    logging.info(f\"Started {process_query_df.__name__}.\")\n",
    "\n",
    "    query_df_processed = reduce(\n",
    "        lambda accu, func: func(accu),\n",
    "        [\n",
    "            partial(\n",
    "                preprocess_query_df,\n",
    "                QUERY_IDENTIFIER=QUERY_IDENTIFIER\n",
    "            ),\n",
    "            partial(\n",
    "                write_queries_to_json,\n",
    "                QUERY_TARGET_PATH=QUERY_TARGET_PATH,\n",
    "                QUERY_IDENTIFIER=QUERY_IDENTIFIER\n",
    "            ),\n",
    "        ],\n",
    "        query_df\n",
    "    )\n",
    "\n",
    "    query_meta_data = extract_query_meta_data(query_df_processed, QUERY_IDENTIFIER)\n",
    "\n",
    "    query_tables = extract_query_tables(query_df_processed, QUERY_IDENTIFIER)\n",
    "\n",
    "    query_columns = extract_query_columns(query_df_processed, QUERY_IDENTIFIER)\n",
    "\n",
    "    write_query_data_to_json(query_meta_data, query_tables, query_columns, QUERY_IDENTIFIER)\n",
    "\n",
    "    logging.info(f\"Completed {process_query_df.__name__} for {QUERY_IDENTIFIER} queries.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### `preprocess_query_df`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_query_df(query_df, QUERY_IDENTIFIER):\n",
    "    \"\"\"\n",
    "    Preprocesses a DataFrame containing SQL queries by performing several cleaning and formatting operations.\n",
    "\n",
    "    Parameters:\n",
    "    query_df (pd.DataFrame): DataFrame containing SQL queries and report names.\n",
    "    QUERY_IDENTIFIER (str): Identifier for the type of queries being processed.\n",
    "\n",
    "    Returns:\n",
    "    pd.DataFrame: A cleaned and formatted DataFrame with duplicate rows removed.\n",
    "\n",
    "    The preprocessing steps include:\n",
    "    - Dropping rows with missing SQL queries and filling other missing values with empty strings.\n",
    "    - Replacing special characters in SQL queries with their ASCII equivalents.\n",
    "    - Formatting SQL queries to uppercase, reindenting, removing comments, and applying specific regex patterns.\n",
    "    - Cleaning the 'Report Name' column using the `clean_string` function.\n",
    "    - Removing duplicate rows from the DataFrame.\n",
    "    \"\"\"\n",
    "    logging.info(f\"Started {preprocess_query_df.__name__} for {QUERY_IDENTIFIER} queries.\")\n",
    "\n",
    "    query_df = query_df.dropna(subset=[\"SQL\"]).fillna(\"\")\n",
    "\n",
    "    query_df[\"SQL\"] = (query_df[\"SQL\"]\n",
    "                       .str.replace('ê', 'e').str.replace('é', 'e').str.replace('è', 'e').str.replace('à', 'a').str.replace('ç', 'c')\n",
    "                       .str.replace('ô', 'o').str.replace('û', 'u').str.replace('ù', 'u').str.replace('î', 'i').str.replace('ï', 'i')\n",
    "                       .str.replace('â', 'a').str.replace('ä', 'a').str.replace('ö', 'o').str.replace('ü', 'u').str.replace('ÿ', 'y')\n",
    "                       .str.replace('ñ', 'n').str.replace('É', 'E').str.replace('È', 'E').str.replace('À', 'A').str.replace('Ç', 'C')\n",
    "                       .str.replace('Ô', 'O').str.replace('Û', 'U').str.replace('Ù', 'U').str.replace('Î', 'I').str.replace('Ï', 'I')\n",
    "                       .str.replace('Â', 'A').str.replace('Ä', 'A').str.replace('Ö', 'O').str.replace('Ü', 'U').str.replace('Ÿ', 'Y')\n",
    "                       .str.replace('Ñ', 'N')\n",
    "    )\n",
    "\n",
    "    pattern_1 = r'WITH\\s+\"\\w+\"\\s+AS\\s*\\(.*?\\)\\s*SELECT'\n",
    "    pattern_2 = r'\"[^\"]*\"\\.'\n",
    "\n",
    "    for index, query in enumerate(query_df[\"SQL\"]):\n",
    "        formatted_query = query.upper()\n",
    "        formatted_query = sqlparse.format(formatted_query, reindent=True, keyword_case='upper', strip_comments=True).strip()\n",
    "        formatted_query = re.sub(pattern_1, 'SELECT', formatted_query, flags=re.DOTALL)\n",
    "        formatted_query = re.sub(pattern_2, '', formatted_query)\n",
    "        query_df.at[index, 'SQL'] = formatted_query\n",
    "\n",
    "    query_df[\"Report Name\"] = query_df[\"Report Name\"].apply(clean_string)\n",
    "\n",
    "    logging.info(f\"Preprocessing of {QUERY_IDENTIFIER} queries successful.\")\n",
    "\n",
    "    return query_df.drop_duplicates()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### `clean_string`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_string(string):\n",
    "    \"\"\"\n",
    "    Cleans a given string by removing specific characters and extra whitespace, and converting it to uppercase.\n",
    "\n",
    "    Parameters:\n",
    "    string (str): The input string to be cleaned.\n",
    "\n",
    "    Returns:\n",
    "    str: The cleaned and formatted string.\n",
    "\n",
    "    The cleaning steps include:\n",
    "    - Removing characters specified in the pattern: square brackets, hash, asterisk, plus, underscore, dot, and digits.\n",
    "    - Stripping leading and trailing whitespace.\n",
    "    - Replacing multiple spaces with a single space.\n",
    "    - Converting the string to uppercase and stripping any remaining leading or trailing whitespace.\n",
    "    \"\"\"\n",
    "    pattern = r'[\\[\\]#*+_.\\d]'\n",
    "    cleaned_string = re.sub(pattern, ' ', string)\n",
    "    cleaned_string = cleaned_string.strip()\n",
    "    cleaned_string = re.sub(r'\\s+', ' ', cleaned_string)\n",
    "    return cleaned_string.upper().strip()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### `write_queries_to_json`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def write_queries_to_json(query_df, QUERY_TARGET_PATH, QUERY_IDENTIFIER):\n",
    "    \"\"\"\n",
    "    Writes SQL queries from a DataFrame to individual JSON files.\n",
    "\n",
    "    Parameters:\n",
    "    query_df (pd.DataFrame): DataFrame containing SQL queries.\n",
    "    QUERY_TARGET_PATH (str): Path where the JSON files will be saved.\n",
    "    QUERY_IDENTIFIER (str): Identifier for the type of queries being processed.\n",
    "\n",
    "    Returns:\n",
    "    pd.DataFrame: The original DataFrame after attempting to write queries to JSON files.\n",
    "\n",
    "    The writing steps include:\n",
    "    - Iterating over the SQL queries in the DataFrame.\n",
    "    - Writing each query to a separate JSON file named with the QUERY_IDENTIFIER and query index.\n",
    "    \"\"\"\n",
    "    logging.info(f\"Started {write_queries_to_json.__name__} for {QUERY_IDENTIFIER} queries.\")\n",
    "\n",
    "    for index, query in enumerate(query_df[\"SQL\"]):\n",
    "        try:\n",
    "            with open(f\"{QUERY_TARGET_PATH}/{QUERY_IDENTIFIER}_query_{index}.sql\", \"w\", encoding='utf-8') as file:\n",
    "                file.write(query)\n",
    "\n",
    "        except Exception as e:\n",
    "            logging.error(f\"Error writing query {index} to JSON-file: {e}\")\n",
    "\n",
    "    logging.info(f\"Queries successfully written to JSON-file.\")\n",
    "\n",
    "    return query_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### `extract_query_meta_data`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_query_meta_data(query_df, QUERY_IDENTIFIER):\n",
    "    \"\"\"\n",
    "    Extracts metadata from a DataFrame containing SQL queries and returns it as a dictionary.\n",
    "\n",
    "    Parameters:\n",
    "    query_df (pd.DataFrame): DataFrame containing SQL queries and associated metadata.\n",
    "    QUERY_IDENTIFIER (str): Identifier for the type of queries being processed.\n",
    "\n",
    "    Returns:\n",
    "    dict: A dictionary containing the extracted metadata for each query.\n",
    "\n",
    "    The extraction steps include:\n",
    "    - Iterating over the rows of the DataFrame.\n",
    "    - Extracting and converting to uppercase the values for 'Datasource', 'Product Name', 'Report Name', and 'Report Path'.\n",
    "    - Using 'nA' as a default value if any of these fields are missing.\n",
    "    - Storing the extracted metadata in a dictionary with keys formatted as 'report_{index}'.\n",
    "    \"\"\"\n",
    "    logging.info(f\"Started {extract_query_meta_data.__name__} from {QUERY_IDENTIFIER} queries.\")\n",
    "\n",
    "    meta_data = {}\n",
    "    for index, (idx, row) in enumerate(query_df.iterrows()):\n",
    "        if \"Datasource\" not in row:\n",
    "            datasource = \"NA\"\n",
    "        else:\n",
    "            datasource = row[\"Datasource\"].upper()\n",
    "        if \"Product Name\" not in row:\n",
    "            product_name = \"NA\"\n",
    "        else:\n",
    "            product_name = row[\"Product Name\"].upper()\n",
    "        if \"Report Name\" not in row:\n",
    "            report_name = \"NA\"\n",
    "        else:\n",
    "            report_name = row[\"Report Name\"].upper()\n",
    "        if \"Report Path\" not in row:\n",
    "            report_path = \"NA\"\n",
    "        else:\n",
    "            report_path = row[\"Report Path\"].upper()\n",
    "\n",
    "        meta_data[report_name] = {\n",
    "                f\"product_name\": product_name,\n",
    "                f\"report_path\": report_path,\n",
    "                f\"datasource\": datasource,\n",
    "            }\n",
    "\n",
    "    logging.info(f\"Meta data from {QUERY_IDENTIFIER} queries successfully extracted.\")\n",
    "\n",
    "    return meta_data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### `extract_query_tables`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_query_tables(query_df, QUERY_IDENTIFIER):\n",
    "    \"\"\"\n",
    "    Extracts table names from SQL queries in a DataFrame and returns them in a structured format.\n",
    "\n",
    "    Parameters:\n",
    "    query_df (pd.DataFrame): DataFrame containing SQL queries.\n",
    "    QUERY_IDENTIFIER (str): Identifier for the type of queries being processed.\n",
    "\n",
    "    Returns:\n",
    "    list: A list of lists, where each inner list contains two lists:\n",
    "          - The first list contains the raw table names extracted from the SQL queries.\n",
    "          - The second list contains the cleansed table names (i.e., without schema prefixes).\n",
    "\n",
    "    The extraction steps include:\n",
    "    - Iterating over the SQL queries in the DataFrame.\n",
    "    - Using a SQL parser to extract table names from each query.\n",
    "    - Converting table names to uppercase and filtering out invalid table names.\n",
    "    - Cleansing table names by removing schema prefixes.\n",
    "    - Sorting the raw and cleansed table names.\n",
    "    - Appending the sorted lists to the `extracted_tables` list.\n",
    "    \"\"\"\n",
    "    logging.info(f\"Started {extract_query_tables.__name__} from {QUERY_IDENTIFIER} queries.\")\n",
    "\n",
    "    INVALID_TABLE_NAMES = [\n",
    "        \"BOTH\",\n",
    "        \"TRIM\",\n",
    "        \"SUBSTR\",\n",
    "        \"SUM\",\n",
    "        \"CAST\",\n",
    "        \"CASE\",\n",
    "        \"TRUNC\",\n",
    "        \"CONCAT\",\n",
    "        \"TO_CHAR\",\n",
    "        \"ROUND\",\n",
    "        \"DISTINCT\",\n",
    "        \"TRAILING\",\n",
    "        \"LENGTH\",\n",
    "        \"BOTH\"\n",
    "    ]\n",
    "\n",
    "    extracted_tables = []\n",
    "\n",
    "    for index, query in query_df[\"SQL\"].items():\n",
    "        try:\n",
    "            raw_query_tables = []\n",
    "            raw_query_tables_cleansed = []\n",
    "\n",
    "            parser = Parser(query)\n",
    "            raw_query_tables = parser.tables\n",
    "            raw_query_tables = [table.upper() for table in raw_query_tables if table.upper() not in INVALID_TABLE_NAMES]\n",
    "            raw_query_tables_cleansed = [table.split(\".\")[-1] for table in raw_query_tables]\n",
    "\n",
    "            raw_query_tables.sort()\n",
    "            raw_query_tables_cleansed.sort()\n",
    "\n",
    "            extracted_tables.append([raw_query_tables, raw_query_tables_cleansed])\n",
    "\n",
    "        except Exception as e:\n",
    "            logging.error(f\"Error extracting table {query}: {e}\")\n",
    "\n",
    "    logging.info(f\"Tables from {QUERY_IDENTIFIER} queries successfully extracted.\")\n",
    "\n",
    "    return extracted_tables"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### `extract_query_columns`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_query_columns(query_df, QUERY_IDENTIFIER):\n",
    "    \"\"\"\n",
    "    Extracts column names from SQL queries in a DataFrame and returns them in a structured format.\n",
    "\n",
    "    Parameters:\n",
    "    query_df (pd.DataFrame): DataFrame containing SQL queries.\n",
    "    QUERY_IDENTIFIER (str): Identifier for the type of queries being processed.\n",
    "\n",
    "    Returns:\n",
    "    list: A list of lists, where each inner list contains two lists:\n",
    "          - The first list contains the raw column names extracted from the SQL queries.\n",
    "          - The second list contains the cleansed column names (i.e., without table prefixes).\n",
    "\n",
    "    The extraction steps include:\n",
    "    - Iterating over the SQL queries in the DataFrame.\n",
    "    - Using a SQL parser to extract column names from each query.\n",
    "    - Cleansing column names by removing table prefixes.\n",
    "    - Sorting and deduplicating the raw and cleansed column names.\n",
    "    - Appending the sorted lists to the `extracted_columns` list.\n",
    "    \"\"\"\n",
    "    logging.info(f\"Started {extract_query_columns.__name__} from {QUERY_IDENTIFIER} queries.\")\n",
    "\n",
    "    extracted_columns = []\n",
    "    error_queries = []\n",
    "\n",
    "    for index, query in query_df[\"SQL\"].items():\n",
    "        try:\n",
    "            processed_query_columns = []\n",
    "            processed_query_columns_cleansed = []\n",
    "\n",
    "            parser = Parser(query)\n",
    "            processed_query_columns = parser.columns\n",
    "            processed_query_columns_cleansed = [col.split(\".\")[-1] for col in processed_query_columns]\n",
    "\n",
    "            processed_query_columns = sorted(set(processed_query_columns))\n",
    "            processed_query_columns_cleansed = sorted(set(processed_query_columns_cleansed))\n",
    "\n",
    "            extracted_columns.append([processed_query_columns, processed_query_columns_cleansed])\n",
    "\n",
    "        except Exception as e:\n",
    "            logging.error(f\"Error extracting columns in query at index {index}: {e}.\")\n",
    "            error_queries.append({'query_identifier': QUERY_IDENTIFIER, 'index': index, 'query': query, 'error': str(e)})\n",
    "\n",
    "    logging.info(f\"Columns from {QUERY_IDENTIFIER} queries successfully extracted.\")\n",
    "\n",
    "    return extracted_columns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### `write_query_data_to_json`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def write_query_data_to_json(meta_data, tables, columns, QUERY_IDENTIFIER):\n",
    "    \"\"\"\n",
    "    Writes query metadata, tables, and columns to a JSON file.\n",
    "\n",
    "    Parameters:\n",
    "    meta_data (dict): Dictionary containing metadata for each query.\n",
    "    tables (list): List of lists containing table names and cleansed table names for each query.\n",
    "    columns (list): List of lists containing column names and cleansed column names for each query.\n",
    "    QUERY_IDENTIFIER (str): Identifier for the type of queries being processed.\n",
    "\n",
    "    Returns:\n",
    "    None\n",
    "\n",
    "    The writing steps include:\n",
    "    - Iterating over the metadata dictionary using report names as keys.\n",
    "    - Updating the metadata with table and column information.\n",
    "    - Writing the updated metadata to a JSON file at the specified path.\n",
    "    \"\"\"\n",
    "    logging.info(f\"Started {write_query_data_to_json.__name__} for {QUERY_IDENTIFIER} queries.\")\n",
    "\n",
    "    JSON_PATH = f\"./4_json_results/{QUERY_IDENTIFIER}_query_data.json\"\n",
    "\n",
    "    for report_name, data in meta_data.items():\n",
    "        index = list(meta_data.keys()).index(report_name)\n",
    "        data[\"tables\"] = tables[index][0]\n",
    "        data[\"tables_cleansed\"] = tables[index][1]\n",
    "        data[\"columns\"] = columns[index][0]\n",
    "        data[\"columns_cleansed\"] = columns[index][1]\n",
    "\n",
    "    with open(JSON_PATH, \"w\", encoding=\"utf-8\") as outfile:\n",
    "        json.dump(meta_data, outfile, indent=4, ensure_ascii=False)\n",
    "\n",
    "    logging.info(f\"{len(meta_data.keys())} queries successfully written to JSON-file. Path: {JSON_PATH}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
